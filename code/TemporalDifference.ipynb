{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da54de8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rl_util in /Users/masha/Desktop/rl-cheatsheet/rl-util (0.0.2)\r\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.9/site-packages (from rl_util) (0.3.0)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (from rl_util) (1.3.4)\r\n",
      "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (1.7.2)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (3.10.0.2)\r\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (1.20.3)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (1.0.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/site-packages (from pandas->rl_util) (2021.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas->rl_util) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/protobuf/3.14.0/libexec/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->rl_util) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install rl_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51b648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import random\n",
    "from rl_util.test import test_policy\n",
    "from rl_util.value import QFunction\n",
    "from rl_util.environment import MarkovEnv\n",
    "from rl_util.policy import EpsSoftPolicy, EpsSoftPolicyFromQ\n",
    "from rl_util.generator import simple_circle\n",
    "import numpy as np\n",
    "\n",
    "S = 'state'\n",
    "A = 'action'\n",
    "R = 'reward'\n",
    "V = 'value'\n",
    "G = 'return'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c079ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsSoftPolicyFromQs(EpsSoftPolicy):\n",
    "    def __init__(self, qs, state_space: int, action_space: int, eps: float):\n",
    "        super().__init__(state_space, action_space, eps)\n",
    "        self.qs = qs\n",
    "\n",
    "    def update(self, s, a):\n",
    "        raise Exception(':(')\n",
    "\n",
    "    def p(self, a, s):\n",
    "        best_a = None\n",
    "        best_v = float('-inf')\n",
    "        for a in range(self.action_space):\n",
    "            cur_v = sum([q.loc[(q[S] == s) & (q[A] == a)][V].values[0] for q in self.qs])\n",
    "            if cur_v > best_v:\n",
    "                best_v = cur_v\n",
    "                best_a = a\n",
    "                \n",
    "        if a == best_a:\n",
    "            return 1 - self.eps + self.eps / self.action_space\n",
    "        else:\n",
    "            return self.eps / self.action_space\n",
    "\n",
    "    def __call__(self, s):\n",
    "        best_a = None\n",
    "        best_v = float('-inf')\n",
    "        for a in range(self.action_space):\n",
    "            cur_v = sum([q.loc[(q[S] == s) & (q[A] == a)][V].values[0] for q in self.qs])\n",
    "            if cur_v > best_v:\n",
    "                best_v = cur_v\n",
    "                best_a = a\n",
    "\n",
    "        probs = [self.eps / self.action_space for _ in range(self.action_space)]\n",
    "        probs[best_a] = 1 - self.eps + self.eps / self.action_space\n",
    "        return random.choices(list(range(self.action_space)), probs, k=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e13375",
   "metadata": {},
   "source": [
    "# SARSA (on-policy TD control)\n",
    "state-action-reward-state-action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603e5392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(alpha, phi, eps, env, iterations):\n",
    "    q = QFunction(env)\n",
    "    policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        action = policy(state)\n",
    "        while not done:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            q_val = q(state, action)\n",
    "            if done:\n",
    "                q_val_next = 0\n",
    "                next_action = None\n",
    "            else:\n",
    "                next_action = policy(next_state)\n",
    "                q_val_next = q(next_state, next_action)\n",
    "            \n",
    "            q.update(state, action, q_val + alpha * (reward + phi * q_val_next - q_val))\n",
    "            state, action = next_state, next_action\n",
    "            policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    return policy, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf26c3",
   "metadata": {},
   "source": [
    "# Q-learning (off-policy TD control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12ae539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(alpha, phi, eps, env, iterations):\n",
    "    q = QFunction(env)\n",
    "    policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            q_val = q(state, action)\n",
    "\n",
    "            if done:\n",
    "                q_val_next = 0\n",
    "            else:\n",
    "                q_val_next = q.q.loc[(q.q[S] == next_state)][V].max()\n",
    "            \n",
    "            q.update(state, action, q_val + alpha * (reward + phi * q_val_next - q_val))\n",
    "            state = next_state\n",
    "            policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    return policy, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38705aeb",
   "metadata": {},
   "source": [
    "# Expected SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2fb584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa(alpha, phi, eps, env, iterations):\n",
    "    q = QFunction(env)\n",
    "    policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            q_val = q(state, action)\n",
    "            \n",
    "            if done:\n",
    "                q_val_next = 0\n",
    "            else:\n",
    "                q_next = q.q.loc[(q.q[S] == next_state)]\n",
    "                q_val_next = 0\n",
    "                for (next_action, value) in zip(q_next[A], q_next[V]):\n",
    "                    q_val_next += policy.p(next_action, state) * value\n",
    "            \n",
    "            q.update(state, action, q_val + alpha * (reward + phi * q_val_next - q_val))\n",
    "            state = next_state\n",
    "            policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    return policy, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fdad6",
   "metadata": {},
   "source": [
    "# Double Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f48c52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_q_learning(alpha, phi, eps, env, iterations):\n",
    "    qs = [QFunction(env), QFunction(env)]\n",
    "    policy = EpsSoftPolicyFromQs([q.q for q in qs], state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            q_index = random.randint(0, 1)\n",
    "            q1, q2 = qs[q_index], qs[1 - q_index]\n",
    "            q_val = q1(state, action)\n",
    "            \n",
    "            if done:\n",
    "                q_val_next = 0\n",
    "            else:\n",
    "                next_action = q1.q.iloc[q1.q.loc[(q1.q[S] == next_state)][V].idxmax()][A]\n",
    "                q_val_next = q2(next_state, next_action)\n",
    "            \n",
    "            q1.update(state, action, q_val + alpha * (reward + phi * q_val_next - q_val))\n",
    "            state = next_state\n",
    "            policy = EpsSoftPolicyFromQs([q.q for q in qs], state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    return policy, qs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd07d0",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3df51ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_circle(state_space=10, action_space=2)\n",
    "alpha = 0.1\n",
    "phi = 0.99\n",
    "eps = 0.5\n",
    "iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2f1b2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>next_state</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state  action  reward  next_state  probability\n",
       "0     0.0     0.0    -3.0         1.0          1.0\n",
       "1     0.0     1.0    -1.0         8.0          1.0\n",
       "2     1.0     0.0    -2.0         2.0          1.0\n",
       "3     1.0     1.0    -1.0         4.0          1.0\n",
       "4     2.0     0.0    -2.0         3.0          1.0\n",
       "5     2.0     1.0    -2.0         4.0          1.0\n",
       "6     3.0     0.0    -3.0         4.0          1.0\n",
       "7     3.0     1.0    -1.0         1.0          1.0\n",
       "8     4.0     0.0    -2.0         5.0          1.0\n",
       "9     4.0     1.0    -1.0         5.0          1.0\n",
       "10    5.0     0.0    -1.0         6.0          1.0\n",
       "11    5.0     1.0    -3.0         9.0          1.0\n",
       "12    6.0     0.0    -1.0         7.0          1.0\n",
       "13    6.0     1.0    -1.0         5.0          1.0\n",
       "14    7.0     0.0    -3.0         8.0          1.0\n",
       "15    7.0     1.0    -3.0         8.0          1.0\n",
       "16    8.0     0.0    -1.0         9.0          1.0\n",
       "17    8.0     1.0    -2.0         0.0          1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c02c8eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 2 steps, reward: -3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy, q = sarsa(alpha, phi, eps, env, iterations)\n",
    "test_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fe940c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 2 steps, reward: -3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 9]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy, q = q_learning(alpha, phi, eps, env, iterations)\n",
    "test_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79376f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 2 steps, reward: -3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 9]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy, q = expected_sarsa(alpha, phi, eps, env, iterations)\n",
    "test_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4716ae0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2x/dlzlkrx57pv_j1qcp6kjgw0w0000gn/T/ipykernel_66400/1757906657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdouble_q_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2x/dlzlkrx57pv_j1qcp6kjgw0w0000gn/T/ipykernel_66400/2433748466.py\u001b[0m in \u001b[0;36mdouble_q_learning\u001b[0;34m(alpha, phi, eps, env, iterations)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mq_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/rl-cheatsheet/rl-util/rl_util/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mfrom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.7/Frameworks/Python.framework/Versions/3.9/lib/python3.9/random.py\u001b[0m in \u001b[0;36mchoices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcum_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The number of weights does not match the population'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcum_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.0\u001b[0m   \u001b[0;31m# convert to float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total of weights must be greater than zero'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "policy, qs = double_q_learning(alpha, phi, eps, env, iterations)\n",
    "test_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec53057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
