{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da54de8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rl_util in /Users/masha/Desktop/rl-cheatsheet/rl-util (0.0.2)\r\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.9/site-packages (from rl_util) (0.3.0)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (from rl_util) (1.3.4)\r\n",
      "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (1.7.2)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (3.10.0.2)\r\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (1.20.3)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (1.0.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/site-packages (from pandas->rl_util) (2021.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas->rl_util) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/protobuf/3.14.0/libexec/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->rl_util) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install rl_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc035091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import random\n",
    "from rl_util.test import test_policy\n",
    "from rl_util.value import QFunction\n",
    "from rl_util.environment import MarkovEnv\n",
    "from rl_util.policy import EpsSoftPolicy, EpsSoftPolicyFromQ\n",
    "from rl_util.generator import simple_circle\n",
    "import numpy as np\n",
    "\n",
    "S = 'state'\n",
    "A = 'action'\n",
    "R = 'reward'\n",
    "V = 'value'\n",
    "G = 'return'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3629df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsSoftPolicyFromQs(EpsSoftPolicy):\n",
    "    def __init__(self, qs, state_space: int, action_space: int, eps: float):\n",
    "        super().__init__(state_space, action_space, eps)\n",
    "        self.qs = qs\n",
    "\n",
    "    def update(self, s, a):\n",
    "        raise Exception(':(')\n",
    "\n",
    "    def p(self, a, s):\n",
    "        best_a = None\n",
    "        best_v = float('-inf')\n",
    "        for a in range(self.action_space):\n",
    "            cur_v = sum([q.loc[(q[S] == s) & (q[A] == a)][V].values[0] for q in self.qs])\n",
    "            if cur_v > best_v:\n",
    "                best_v = cur_v\n",
    "                best_a = a\n",
    "                \n",
    "        if a == best_a:\n",
    "            return 1 - self.eps + self.eps / self.action_space\n",
    "        else:\n",
    "            return self.eps / self.action_space\n",
    "\n",
    "    def __call__(self, s):\n",
    "        best_a = None\n",
    "        best_v = float('-inf')\n",
    "        for a in range(self.action_space):\n",
    "            cur_v = sum([q.loc[(q[S] == s) & (q[A] == a)][V].values[0] for q in self.qs])\n",
    "            if cur_v > best_v:\n",
    "                best_v = cur_v\n",
    "                best_a = a\n",
    "        probs = [self.eps / self.action_space for _ in range(self.action_space)]\n",
    "        probs[best_a] = 1 - self.eps + self.eps / self.action_space\n",
    "        return random.choices(list(range(self.action_space)), probs, k=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c384cb9",
   "metadata": {},
   "source": [
    "# SARSA (on-policy TD control)\n",
    "state-action-reward-state-action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8322f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(alpha, phi, eps, env, iterations):\n",
    "    q = QFunction(env)\n",
    "    policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        action = policy(state)\n",
    "        while not done:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            q_val = q(state, action)\n",
    "            if done:\n",
    "                q_val_next = 0\n",
    "                next_action = None\n",
    "            else:\n",
    "                next_action = policy(next_state)\n",
    "                q_val_next = q(next_state, next_action)\n",
    "            \n",
    "            q.update(state, action, q_val + alpha * (reward + phi * q_val_next - q_val))\n",
    "            state, action = next_state, next_action\n",
    "            policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    return policy, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d19006",
   "metadata": {},
   "source": [
    "# Q-learning (off-policy TD control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd2f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(alpha, phi, eps, env, iterations):\n",
    "    q = QFunction(env)\n",
    "    policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            q_val = q(state, action)\n",
    "\n",
    "            if done:\n",
    "                q_val_next = 0\n",
    "            else:\n",
    "                q_val_next = q.q.loc[(q.q[S] == next_state)][V].max()\n",
    "            \n",
    "            q.update(state, action, q_val + alpha * (reward + phi * q_val_next - q_val))\n",
    "            state = next_state\n",
    "            policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    return policy, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc501ab5",
   "metadata": {},
   "source": [
    "# Expected SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d759f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa(alpha, phi, eps, env, iterations):\n",
    "    q = QFunction(env)\n",
    "    policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            q_val = q(state, action)\n",
    "            \n",
    "            if done:\n",
    "                q_val_next = 0\n",
    "            else:\n",
    "                q_next = q.q.loc[(q.q[S] == next_state)]\n",
    "                q_val_next = 0\n",
    "                for (next_action, value) in zip(q_next[A], q_next[V]):\n",
    "                    q_val_next += policy.p(next_action, state) * value\n",
    "            \n",
    "            q.update(state, action, q_val + alpha * (reward + phi * q_val_next - q_val))\n",
    "            state = next_state\n",
    "            policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    return policy, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18041171",
   "metadata": {},
   "source": [
    "# Double Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ccabc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_q_learning(alpha, phi, eps, env, iterations):\n",
    "    qs = [QFunction(env), QFunction(env)]\n",
    "    policy = EpsSoftPolicyFromQs([q.q for q in qs], state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            q_index = random.randint(0, 1)\n",
    "            q1, q2 = qs[q_index], qs[1 - q_index]\n",
    "            q_val = q1(state, action)\n",
    "            \n",
    "            if done:\n",
    "                q_val_next = 0\n",
    "            else:\n",
    "                next_action = q1.q.iloc[q1.q.loc[(q1.q[S] == next_state)][V].idxmax()][A]\n",
    "                q_val_next = q2(next_state, next_action)\n",
    "            \n",
    "            q1.update(state, action, q_val + alpha * (reward + phi * q_val_next - q_val))\n",
    "            state = next_state\n",
    "            policy = EpsSoftPolicyFromQs([q.q for q in qs], state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    return policy, qs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd1bab",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6241810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_circle(state_space=10, action_space=2)\n",
    "alpha = 0.1\n",
    "phi = 0.99\n",
    "eps = 0.5\n",
    "iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84b2d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>next_state</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state  action  reward  next_state  probability\n",
       "0     0.0     0.0    -3.0         1.0          1.0\n",
       "1     0.0     1.0    -1.0         8.0          1.0\n",
       "2     1.0     0.0    -2.0         2.0          1.0\n",
       "3     1.0     1.0    -1.0         4.0          1.0\n",
       "4     2.0     0.0    -2.0         3.0          1.0\n",
       "5     2.0     1.0    -2.0         4.0          1.0\n",
       "6     3.0     0.0    -3.0         4.0          1.0\n",
       "7     3.0     1.0    -1.0         1.0          1.0\n",
       "8     4.0     0.0    -2.0         5.0          1.0\n",
       "9     4.0     1.0    -1.0         5.0          1.0\n",
       "10    5.0     0.0    -1.0         6.0          1.0\n",
       "11    5.0     1.0    -3.0         9.0          1.0\n",
       "12    6.0     0.0    -1.0         7.0          1.0\n",
       "13    6.0     1.0    -1.0         5.0          1.0\n",
       "14    7.0     0.0    -3.0         8.0          1.0\n",
       "15    7.0     1.0    -3.0         8.0          1.0\n",
       "16    8.0     0.0    -1.0         9.0          1.0\n",
       "17    8.0     1.0    -2.0         0.0          1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5a141e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 2 steps, reward: -2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 8, 9]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy, q = sarsa(alpha, phi, eps, env, iterations)\n",
    "test_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "511ce0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 7 steps, reward: -13.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 1, 4, 5, 9]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy, q = q_learning(alpha, phi, eps, env, iterations)\n",
    "test_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "622a0927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 5 steps, reward: -11.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 4, 5, 9]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy, q = expected_sarsa(alpha, phi, eps, env, iterations)\n",
    "test_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "476efd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 2 steps, reward: -2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 8, 9]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy, qs = double_q_learning(alpha, phi, eps, env, iterations)\n",
    "test_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f4ca7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
