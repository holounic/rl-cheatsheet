{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d029b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc1fad1",
   "metadata": {},
   "source": [
    "## Markov environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a952a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Markov:\n",
    "    def __init__(self, transitions: dict, seed: int = 11):\n",
    "        self.transitions = transitions\n",
    "        self.state_space = len(self.transitions)\n",
    "        self.transition_probs = {}\n",
    "        self.rewards = {}\n",
    "        \n",
    "        key = random.PRNGKey(seed)\n",
    "        for state in self.transitions.keys():\n",
    "            self.rewards[state] = random.normal((key:=random.split(key)[0]), (1,))\n",
    "            \n",
    "            self.transition_probs[state] = {}\n",
    "            for action in self.transitions[state]:\n",
    "                next_states = self.transitions[state][action]\n",
    "                self.transition_probs[state][action] = {}\n",
    "                state_probs = random.randint((key:=random.split(key)[0]), (len(next_states),), 1, 2 * len(next_states))\n",
    "                state_probs = state_probs / state_probs.sum()\n",
    "                for (next_state, state_prob) in zip(next_states, state_probs):\n",
    "                    self.transition_probs[state][action][next_state] = state_prob   \n",
    "    \n",
    "    def states(self):\n",
    "        return list(self.transitions.keys())\n",
    "    \n",
    "    def state_space(self):\n",
    "        return self.state_space\n",
    "    \n",
    "    def actions(self, state):\n",
    "        return list(self.transition_probs[state].keys())\n",
    "    \n",
    "    def next_states(self, state):\n",
    "        states = []\n",
    "        for action in self.transition_probs[state].keys():\n",
    "            states = states + list(self.transition_probs[state][action].keys())\n",
    "        return list(set(states))\n",
    "    \n",
    "    def p(self, state, action, _, next_state):\n",
    "        return self.transition_probs.get(state, {action: {next_state: 0}}).get(action, {next_state:0}).get(next_state, 0)\n",
    "    \n",
    "    def rewards(self, state):\n",
    "        return self.rewards[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a768c2",
   "metadata": {},
   "source": [
    "## Policy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31727dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def p(self, a, s):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class StochasticPolicy(Policy):\n",
    "    def __init__(self, state_actions: dict, seed: int = 11):\n",
    "        key = random.PRNGKey(seed)\n",
    "        self.probs = {}\n",
    "    \n",
    "        for state in state_actions:\n",
    "            self.probs[state] = {}\n",
    "            actions = state_actions[state]\n",
    "        \n",
    "            action_probs = random.randint((key:=random.split(key)[0]), (len(actions),), 1, 2 * len(actions) + 1)\n",
    "            action_probs = action_probs / action_probs.sum()\n",
    "        \n",
    "            for (action, prob) in zip(actions, action_probs):\n",
    "                self.probs[state][action] = prob\n",
    "    \n",
    "    def p(self, a, s):\n",
    "        return self.probs[s][a]\n",
    "    \n",
    "    def __call__(self, s):\n",
    "        probs = self.probs[state]\n",
    "        max_a = probs.keys()[0]\n",
    "        for a in probs.keys():\n",
    "            if probs[a] > probs[max_a]:\n",
    "                max_a = a\n",
    "        return max_a\n",
    "    \n",
    "class DeterministicPolicy(Policy):\n",
    "    def __init__(self, transitions: dict):\n",
    "        self.transitions = transitions\n",
    "        \n",
    "    def p(self, a, s):\n",
    "        return 1. if self.transitions[a] == s else 0\n",
    "    \n",
    "    def __call__(self, s):\n",
    "        return self.transitions[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d811d",
   "metadata": {},
   "source": [
    "## Policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee6f2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, markov, theta: float, phi: float):\n",
    "    v = jnp.zeros(markov.state_space)\n",
    "    delta = float('inf')\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s in markov.states():\n",
    "            old_v = v[s]\n",
    "            v_s = 0.\n",
    "            a = policy(s)\n",
    "            for s_dot in markov.next_states(s):\n",
    "                for r in markov.rewards[s_dot]:\n",
    "                    v_s += markov.p(s, a, r, s_dot) * (r + phi * v[s_dot])\n",
    "            v = v.at[s].set(v_s)\n",
    "            delta = max(delta, abs(v_s - old_v))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821a7e5",
   "metadata": {},
   "source": [
    "## Policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e8826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(markov, v, phi: float, policy=None):\n",
    "    if policy is None:\n",
    "        policy = DeterministicPolicy({state: markov.actions(state)[0] for state in markov.states()})\n",
    "    \n",
    "    policy_stable = True\n",
    "    for s in markov.states():\n",
    "        old_action = policy(s)\n",
    "        max_a, max_value = markov.actions(s)[0], float('-inf')\n",
    "        for a in markov.actions(s):\n",
    "            cur_value = 0\n",
    "            for s_dot in markov.next_states(s):\n",
    "                    for r in markov.rewards[s_dot]:\n",
    "                        cur_value += markov.p(s, a, r, s_dot) * (r + phi * v[s_dot])\n",
    "            if cur_value > max_value:\n",
    "                max_a = a\n",
    "                max_value = cur_value\n",
    "        if old_action == max_a:\n",
    "            continue\n",
    "        policy.transitions[s] = max_a\n",
    "        policy_stable = False\n",
    "    return policy, policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed058e",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7fd0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(markov, theta: float, phi: float):\n",
    "    policy = DeterministicPolicy({state: markov.actions(state)[0] for state in markov.states()})\n",
    "    \n",
    "    while True:\n",
    "        v = policy_evaluation(policy, markov, theta, phi)\n",
    "        policy, policy_stable = policy_improvement(markov, v, phi, policy)\n",
    "        if policy_stable:\n",
    "            return policy, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe15ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {1: {1: DeviceArray(1., dtype=float32)},\n",
       "  3: {3: DeviceArray(1., dtype=float32)}},\n",
       " 1: {1: {0: DeviceArray(1., dtype=float32)},\n",
       "  2: {1: DeviceArray(1., dtype=float32)}},\n",
       " 2: {3: {0: DeviceArray(1., dtype=float32)},\n",
       "  2: {4: DeviceArray(1., dtype=float32)}},\n",
       " 3: {1: {2: DeviceArray(1., dtype=float32)}},\n",
       " 4: {1: {1: DeviceArray(1., dtype=float32)},\n",
       "  2: {5: DeviceArray(1., dtype=float32)}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s -> a -> s_dot\n",
    "# -- 0 -> 1, 3 --\n",
    "# 0 - > 1 -> 1\n",
    "# 0 -> 3 -> 3\n",
    "# -- 1 -> 0, 4 --\n",
    "# 1 -> 1 -> 0\n",
    "# 1 -> 2 -> 4\n",
    "# -- 2 -> 0, 4 --\n",
    "# 2 -> 3 -> 0\n",
    "# 2 -> 2 -> 4\n",
    "# -- 3 -> 2\n",
    "# 3 -> 1 -> 2\n",
    "# -- 4 -> 1, 3, 5(terminal)\n",
    "# 4 -> 1 -> 1\n",
    "# 4 -> 2 -> 5(terminal)\n",
    "\n",
    "transitions = {\n",
    "    0 : {1 : [1], 3: [3]},\n",
    "    1 : {1: [0], 2: [1]}, \n",
    "    2 : {3: [0], 2: [4]}, \n",
    "    3 : {1: [2]}, \n",
    "    4 : {1: [1], 2: [5]}\n",
    "}\n",
    "\n",
    "markov = Markov(transitions)\n",
    "markov.rewards = {0: [-1], 1: [-1], 2: [-2], 3: [-1], 4: [-1], 5: [0]}\n",
    "\n",
    "markov.transition_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b32c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.9\n",
    "phi = 0.99\n",
    "\n",
    "policy, value = policy_iteration(markov, theta, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b0e93a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3, 1: 1, 2: 2, 3: 1, 4: 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12f8370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-3.9601  , -4.920499, -1.      , -2.99    ,  0.      ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c453130d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [3, 2, 4, 5], 1: [0, 3, 2, 4, 5], 2: [4, 5], 3: [2, 4, 5], 4: [5]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminal = 5\n",
    "trajectories = {}\n",
    "\n",
    "for start in markov.states():\n",
    "    s = start\n",
    "    trajectory = []\n",
    "    while s != terminal:\n",
    "        s = markov.transitions[s][policy(s)][0]\n",
    "        trajectory.append(s)\n",
    "    trajectories[start] = trajectory\n",
    "\n",
    "trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72dd67d",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7cc5f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(markov, theta: float):\n",
    "    v = jnp.zeros(markov.state_space)\n",
    "    delta = float('inf')\n",
    "    \n",
    "    transition = {s: None for s in markov.states()}\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s in markov.states():\n",
    "            old_v = v[s]\n",
    "            max_a, max_val = markov.actions(s)[0], float('-inf')\n",
    "            for a in markov.actions(s):\n",
    "                cur_value = 0\n",
    "                for s_dot in markov_state.next_states(s):\n",
    "                        for r in markov.rewards(next_s):\n",
    "                            cur_value += markov.p(s, a, r, s_dot) * (r + phi * v[s_dot])\n",
    "                if cur_value > max_value:\n",
    "                    max_a = a\n",
    "                    max_value = cur_value\n",
    "            transitions[s] = max_a\n",
    "            v = v.at[s].set(max_value)\n",
    "            delta = max(delta, abs(old_v - v[s]))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a3d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
