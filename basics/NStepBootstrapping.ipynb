{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c613c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rl_util in /Users/masha/Desktop/rl-cheatsheet/rl-util (0.0.2)\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.9/site-packages (from rl_util) (0.3.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (from rl_util) (1.3.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (3.10.0.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (1.7.2)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (3.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas->rl_util) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/site-packages (from pandas->rl_util) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/protobuf/3.14.0/libexec/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->rl_util) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install rl_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587ae114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import random\n",
    "from rl_util.test import test_policy\n",
    "from rl_util.value import QFunction\n",
    "from rl_util.environment import MarkovEnv\n",
    "from rl_util.policy import EpsSoftPolicy, EpsSoftPolicyFromQ, GreedyPolicyFromQ\n",
    "from rl_util.generator import simple_circle\n",
    "import numpy as np\n",
    "\n",
    "S = 'state'\n",
    "A = 'action'\n",
    "R = 'reward'\n",
    "V = 'value'\n",
    "G = 'return'\n",
    "NS = 'next_step'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06dfd27",
   "metadata": {},
   "source": [
    "# On-policy N-step SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "38f565fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_sarsa(n, alpha, phi, eps, env, iterations):\n",
    "    q = QFunction(env)\n",
    "    policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        action = policy(state)\n",
    "        T = float('inf')\n",
    "        done = False\n",
    "        t = 0\n",
    "        trace = [{S: state, A: action, R: 0}]\n",
    "        while True:\n",
    "            if t < T:\n",
    "                next_state, reward, done = env.step(action)\n",
    "                if done:\n",
    "                    T = t + 1\n",
    "                    action = None\n",
    "                else:\n",
    "                    action = policy(next_state)\n",
    "                    state = next_state\n",
    "                trace.append({R: reward, A: action, S: state})\n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                g = sum([trace[i][R] * (phi ** (i - tau - 1)) for i in range(tau + 1, min(T, tau + n))])\n",
    "                if tau + n < T:\n",
    "                    g = g + (phi ** n) * q(trace[tau + n][S], trace[tau + n][A])\n",
    "                q.update(trace[tau][S], trace[tau][A], q(trace[tau][S], trace[tau][A]) + alpha * (g - q(trace[tau][S], trace[tau][A])))\n",
    "                policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "            if tau + 1 == T:\n",
    "                break\n",
    "            t += 1\n",
    "    return GreedyPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space()), q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f39fe93",
   "metadata": {},
   "source": [
    "# Off-policy N-step SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9f7911b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_n_step_sarsa(b_policy, n, alpha, phi, eps, env, iterations):\n",
    "    q = QFunction(env)\n",
    "    target_policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        action = b_policy(state)\n",
    "        T = float('inf')\n",
    "        done = False\n",
    "        t = 0\n",
    "        trace = [{R: 0, S: state, A: action}]\n",
    "        while True:\n",
    "            if t < T:\n",
    "                next_state, reward, done = env.step(action)\n",
    "                if done:\n",
    "                    T = t + 1\n",
    "                    action = None\n",
    "                else:\n",
    "                    action = b_policy(next_state)\n",
    "                    state = next_state\n",
    "                trace.append({R: reward, A: action, S: state})\n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                ro = 1\n",
    "                for i in range(tau + 1,  min(T - 1, tau + n - 1)):\n",
    "                    ro *= (target_policy.p(trace[i][A], trace[i][S]) / b_policy.p(trace[i][A], trace[i][S])) # importance sampling ratio\n",
    "                g = sum([trace[i][R] * (phi ** (i - tau - 1)) for i in range(tau + 1, min(T, tau + n))])\n",
    "                if tau + n < T and tau + n < len(trace):\n",
    "                    g = g + (phi ** n) * q(trace[tau + n][S], trace[tau + n][A])\n",
    "                q.update(trace[tau][S], trace[tau][A], q(trace[tau][S], trace[tau][A]) + alpha * ro * (g - q(trace[tau][S], trace[tau][A])))\n",
    "            target_policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "            if tau + 1 == T:\n",
    "                break\n",
    "            t += 1\n",
    "    return GreedyPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space()), q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232eb42",
   "metadata": {},
   "source": [
    "# Tree Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2efc808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_backup(n, alpha, phi, eps, env, iterations):\n",
    "    q = QFunction(env)\n",
    "    policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        action = policy(state)\n",
    "        T = float('inf')\n",
    "        done = False\n",
    "        t = 0\n",
    "        trace = [{R: 0, S: state, A: action}]\n",
    "        while True:\n",
    "            if t < T:\n",
    "                next_state, reward, done = env.step(action)\n",
    "                if done:\n",
    "                    T = t + 1\n",
    "                    action = None\n",
    "                else:\n",
    "                    action = random.randint(0, env.action_space() - 1)\n",
    "                    state = next_state\n",
    "                trace.append({R: reward, A: action, S: state})\n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                if t + 1 >= T:\n",
    "                    g = trace[T][R]\n",
    "                else:\n",
    "                    g = trace[t + 1][R] + phi * sum([policy.p(a, trace[t + 1][S]) * q(trace[t + 1][S], a) for a in range(env.action_space())])\n",
    "                for k in range(min(t, T - 1), tau, -1):\n",
    "                    g = trace[k][R] + phi * (sum([policy.p(a, trace[k][S]) * q(trace[k][S], a) * (0 if a == trace[k][A] else 1) for a in range(env.action_space())]) + policy.p(trace[k][A], trace[k][S]) * q(trace[k][S], trace[k][A]) * g)\n",
    "                q.update(\n",
    "                    trace[tau][S], \n",
    "                    trace[tau][A], \n",
    "                    q(trace[tau][S], trace[tau][A]) + alpha * (g - q(trace[tau][S], trace[tau][A]))\n",
    "                )\n",
    "            policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "            if tau + 1 == T:\n",
    "                break\n",
    "            t += 1\n",
    "    return GreedyPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space()), q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896fe0f3",
   "metadata": {},
   "source": [
    "# N-step Q(omega) with degree of sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "82572baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_q_omega(b_policy, n, alpha, phi, eps, env, iterations):\n",
    "    q = QFunction(env)\n",
    "    target_policy = EpsSoftPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        action = b_policy(state)\n",
    "        T = float('inf')\n",
    "        done = False\n",
    "        t = 0\n",
    "        omegas, ros = [0], [0]\n",
    "        trace = [{R: 0, S: state, A: action}]\n",
    "        while True:\n",
    "            if t < T:\n",
    "                next_state, reward, done = env.step(action)\n",
    "                if done:\n",
    "                    T = t + 1\n",
    "                    action = None\n",
    "                else:\n",
    "                    action = b_policy(next_state)\n",
    "                    state = next_state\n",
    "                    \n",
    "                    omega = random.uniform(0, 1)\n",
    "                    omegas.append(omega)\n",
    "                    ro = target_policy.p(action, state) / b_policy.p(action, state)\n",
    "                    ros.append(ro)\n",
    "                trace.append({R: reward, A: action, S: state})\n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                g = 0\n",
    "                for k in range(min(t + 1, T), tau, -1):\n",
    "                    if k == T:\n",
    "                        g = trace[k][R]\n",
    "                    else:\n",
    "                        v = sum([target_policy.p(a, trace[k][S]) * q(trace[k][S], a) for a in range(env.action_space())])\n",
    "                        g = trace[k][R] + phi * (ros[k] * omegas[k] + (1 - omegas[k]) * target_policy.p(trace[k][A], trace[k][S])) * (g - q(trace[k][S], trace[k][A])) + phi * v\n",
    "                q.update(\n",
    "                    trace[tau][S], \n",
    "                    trace[tau][A], \n",
    "                    q(trace[tau][S], trace[tau][A]) + alpha * (g - q(trace[tau][S], trace[tau][A]))\n",
    "                )\n",
    "            target_policy = EpsSoftPolicyFromQ(q.q, eps=eps, state_space=env.state_space(), action_space=env.action_space())\n",
    "            if tau + 1 == T:\n",
    "                break\n",
    "            t += 1\n",
    "    return GreedyPolicyFromQ(q.q, state_space=env.state_space(), action_space=env.action_space()), q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b1dabf",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7e6205bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_circle(state_space=4, action_space=2)\n",
    "alpha = 0.1\n",
    "phi = 0.99\n",
    "eps = 0.5\n",
    "iterations = 15\n",
    "n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f360414c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>next_state</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state  action  reward  next_state  probability\n",
       "0    0.0     0.0    -1.0         1.0          1.0\n",
       "1    0.0     1.0    -2.0         3.0          1.0\n",
       "2    1.0     0.0    -3.0         2.0          1.0\n",
       "3    1.0     1.0    -3.0         0.0          1.0\n",
       "4    2.0     0.0    -3.0         3.0          1.0\n",
       "5    2.0     1.0    -3.0         2.0          1.0"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ee058afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 1 steps, reward: -2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 3]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On-policy sarsa\n",
    "nss_policy, q = n_step_sarsa(n, alpha, phi, eps, env, iterations)\n",
    "test_policy(env, nss_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4fbe2cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 1 steps, reward: -2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 3]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Off-policy sarsa\n",
    "opnss_policy, q = off_policy_n_step_sarsa(nss_policy, n, alpha, phi, eps, env, 8)\n",
    "test_policy(env, opnss_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "06b37a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 1 steps, reward: -2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 3]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tree backup\n",
    "tb_policy, q = tree_backup(n, alpha, phi, eps, env, 1)\n",
    "test_policy(env, tb_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b13f33be",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ro' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2x/dlzlkrx57pv_j1qcp6kjgw0w0000gn/T/ipykernel_76614/2552352169.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Q(omega)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mqo_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_step_q_omega\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnss_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqo_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2x/dlzlkrx57pv_j1qcp6kjgw0w0000gn/T/ipykernel_76614/3562510978.py\u001b[0m in \u001b[0;36mn_step_q_omega\u001b[0;34m(b_policy, n, alpha, phi, eps, env, iterations)\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mtrace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mtrace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0mq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 )\n\u001b[1;32m     41\u001b[0m             \u001b[0mtarget_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpsSoftPolicyFromQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'ro' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Q(omega)\n",
    "qo_policy, q = n_step_q_omega(nss_policy, 10, alpha, phi, eps, env, 1)\n",
    "test_policy(env, qo_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186f3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
