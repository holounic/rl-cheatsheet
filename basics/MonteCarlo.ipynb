{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a0720a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rl_util in /Users/masha/Desktop/rl-cheatsheet/rl-util (0.0.2)\r\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.9/site-packages (from rl_util) (0.3.0)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (from rl_util) (1.3.4)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (1.0.0)\r\n",
      "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (1.7.2)\r\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (3.3.0)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (3.10.0.2)\r\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/site-packages (from jax->rl_util) (1.20.3)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/site-packages (from pandas->rl_util) (2021.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas->rl_util) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/protobuf/3.14.0/libexec/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->rl_util) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install rl_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b81c92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import random\n",
    "import rl_util\n",
    "from rl_util.test import test_policy\n",
    "from rl_util.environment import MarkovEnv\n",
    "from rl_util.policy import DeterministicPolicy, EpsSoftPolicy\n",
    "from rl_util.generator import simple_circle\n",
    "import random as std_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "554dd7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 'state'\n",
    "A = 'action'\n",
    "R = 'reward'\n",
    "V = 'value'\n",
    "G = 'return'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb614316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_episode_generator(episode_len=10, n_states=10, n_actions=3):\n",
    "    def f(env, _):\n",
    "        return [{S : std_rand.randrange(n_states), \n",
    "                 A : std_rand.randrange(n_actions), \n",
    "                 R : std_rand.gauss(0, 1)} for _ in range(episode_len)]\n",
    "    return f\n",
    "\n",
    "\n",
    "def acting_episode_generator(max_episode_len=10):\n",
    "    def f(env, policy):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done and t < max_episode_len:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append(\n",
    "                {S: state, \n",
    "                 A: action, \n",
    "                 R: reward})\n",
    "            state = next_state\n",
    "            t += 1\n",
    "        return episode\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdd873c",
   "metadata": {},
   "source": [
    "# On-policy Monte Carlo prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "504666ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_monte_carlo_prediction(policy, phi, env, iterations, episode_generator, first_visit=True):\n",
    "    v = jnp.ones((len(env.states()), ))\n",
    "    returns = [[] for _ in env.states()]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        episode = episode_generator(env, policy)\n",
    "        g = 0\n",
    "        used_s = set()\n",
    "        for T in range(len(episode)):\n",
    "            for t in range(T - 1, -1, -1):\n",
    "                g = phi * g + episode[t][R]\n",
    "                s = episode[t][S]\n",
    "                if not first_visit or s not in used_s:\n",
    "                    returns[s].append(g)\n",
    "                    v = v.at[s].set(sum(returns[s]) / len(returns[s]))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a34a49",
   "metadata": {},
   "source": [
    "# On-policy Monte Carlo control with eps-soft policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ce5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_monte_carlo_control(phi, eps, env, iterations, episode_generator, first_visit=True):\n",
    "    policy = EpsSoftPolicy(state_space=env.state_space(), action_space=env.action_space(), eps=eps)\n",
    "    returns = pd.DataFrame({S : [], A : [], G : []})\n",
    "    q = pd.DataFrame({S : [], A : [], V : []})\n",
    "    for i in range(iterations): \n",
    "        episode = episode_generator(env, policy)\n",
    "        \n",
    "        first_appeared = {}\n",
    "        for t in range(len(episode)):\n",
    "            step = episode[t]\n",
    "            state, action, reward = step[S], step[A], step[R]\n",
    "            if (state, action) not in first_appeared:\n",
    "                first_appeared[(state, action)] = t\n",
    "        \n",
    "        g = 0\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            step = episode[t]\n",
    "            state, action, reward = step[S], step[A], step[R]\n",
    "            g = phi * g + reward\n",
    "            if not first_visit or first_appeared.get((state, action), None) == t:\n",
    "                returns = returns.append({S : state, A : action, G : g}, ignore_index=True)\n",
    "                average_return = returns.loc[(returns[S] == state) & (returns[A] == action)][G].mean()\n",
    "                if len(q.loc[(q[S] == state) & (q[A] == action)]) == 0:\n",
    "                    q = q.append({S : state, A : action, V : average_return}, ignore_index=True)\n",
    "                else:\n",
    "                    q.loc[(q[S] == state) & (q[A] == action), V] = average_return\n",
    "                a_max_idx = q.loc[q[S] == state][V].idxmax()\n",
    "                a_best = q.iloc[a_max_idx][A]\n",
    "                policy.update(state, a_best)\n",
    "    return policy, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dd8538",
   "metadata": {},
   "source": [
    "# Off-policy Monte Carlo prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45f5ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_monte_carlo_prediction(b_policy, t_policy, phi, env, iterations, episode_generator):\n",
    "    returns = pd.DataFrame({S : [], A : [], R : []})\n",
    "    q = pd.DataFrame({S : [], A : [], V : []})\n",
    "    c = pd.DataFrame({S : [], A : [], V : []})\n",
    "    for _ in range(iterations):\n",
    "        episode = episode_generator(env, policy)\n",
    "        g = 0\n",
    "        w = 1\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            if w == 0:\n",
    "                break\n",
    "            step = episode[t]\n",
    "            state, action, reward = step[S], step[A], step[R]\n",
    "            g = phi * g + reward\n",
    "            \n",
    "            if len(c.loc[(c[S] == state) & (c[A] == action)]) == 0:\n",
    "                c = c.append({S : state, A : action, V : w}, ignore_index=True)\n",
    "            else:\n",
    "                c.loc[(c[S] == state) & (c[A] == action), V] += w\n",
    "\n",
    "            if len(q.loc[(q[S] == state) & (q[A] == action)]) == 0:\n",
    "                q = q.append({S : state, A : action, V : 0}, ignore_index=True)\n",
    "                \n",
    "            cur_c = c.loc[(c[S] == state) & (c[A] == action)][V]\n",
    "            cur_q = q.loc[(q[S] == state) & (q[A] == action)][V]\n",
    "            \n",
    "            q.loc[(q[S] == state) & (q[A] == action), V] = (cur_q + (g - cur_q) * w / cur_c).values[0]\n",
    "            w = w * t_policy.p(action, state) / b_policy.p(action, state)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ea4ed",
   "metadata": {},
   "source": [
    "# Off-policy Monte Carlo control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b9339e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_monte_carlo_control(b_policy, phi, env, iterations, episode_generator):\n",
    "    t_policy = DeterministicPolicy(state_space=env.state_space(), action_space=env.action_space())\n",
    "    q = pd.DataFrame({S : [], A : [], V : []})\n",
    "    c = pd.DataFrame({S : [], A : [], V : []})\n",
    "    for _ in range(iterations):\n",
    "        episode = episode_generator(env, policy)\n",
    "        g = 0\n",
    "        w = 1\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            step = episode[t]\n",
    "            state, action, reward = step[S], step[A], step[R]\n",
    "            g = phi * g + reward\n",
    "            \n",
    "            if len(c.loc[(c[S] == state) & (c[A] == action)]) == 0:\n",
    "                c = c.append({S : state, A : action, V : w}, ignore_index=True)\n",
    "            else:\n",
    "                c.loc[(c[S] == state) & (c[A] == action), V] += w\n",
    "\n",
    "            if len(q.loc[(q[S] == state) & (q[A] == action)]) == 0:\n",
    "                q = q.append({S : state, A : action, V : 0}, ignore_index=True)\n",
    "                \n",
    "            cur_c = c.loc[(c[S] == state) & (c[A] == action)][V]\n",
    "            cur_q = q.loc[(q[S] == state) & (q[A] == action)][V]\n",
    "            q.loc[(q[S] == state) & (q[A] == action)][V] = cur_q +  (g - cur_q) * w / cur_c\n",
    "            \n",
    "            a_max_idx = q.loc[q[S] == state][V].idxmax()\n",
    "            a_best = q.iloc[a_max_idx][A]\n",
    "            t_policy.update(state, a_best)\n",
    "            \n",
    "            if not action == a_best:\n",
    "                break\n",
    "                \n",
    "            w = w * 1 / b_policy.p(action, state)\n",
    "    return t_policy, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da74d2f2",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dab0a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = 2\n",
    "state_space = 10\n",
    "\n",
    "phi = .99\n",
    "eps = 0.5\n",
    "iterations = 50\n",
    "\n",
    "env = simple_circle(state_space=state_space, action_space=action_space)\n",
    "policy, q = on_policy_monte_carlo_control(phi, eps, env, iterations, acting_episode_generator(), first_visit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ae42b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 8 steps, reward: -15.0\n"
     ]
    }
   ],
   "source": [
    "test_policy(env, policy, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba4f50bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-16.67142 , -30.36178 , -36.140907, -33.573757, -37.87401 ,\n",
       "             -37.681473, -35.706627, -18.78731 , -22.580856],            dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_policy_monte_carlo_prediction(policy, phi, env, iterations, acting_episode_generator(), first_visit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "844b4f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2x/dlzlkrx57pv_j1qcp6kjgw0w0000gn/T/ipykernel_62642/3300999621.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  q.loc[(q[S] == state) & (q[A] == action)][V] = cur_q + w / cur_c  * (g - cur_q)\n"
     ]
    }
   ],
   "source": [
    "target_policy, q = off_policy_monte_carlo_control(policy, phi, env, iterations, acting_episode_generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9336056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 3 steps, reward: -5.0\n"
     ]
    }
   ],
   "source": [
    "test_policy(env, target_policy, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e1688bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.854286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.539934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.930300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.739096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.756118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.670171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.633470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.587135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-10.733965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.880995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-11.792185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state  action      value\n",
       "0     8.0     0.0  -3.000000\n",
       "1     7.0     0.0  -3.854286\n",
       "2     6.0     0.0  -5.539934\n",
       "3     0.0     1.0  -4.930300\n",
       "4     7.0     1.0  -4.739096\n",
       "5     6.0     1.0  -5.970000\n",
       "6     3.0     1.0  -7.756118\n",
       "7     5.0     1.0  -3.670171\n",
       "8     4.0     0.0  -4.633470\n",
       "9     3.0     0.0  -7.587135\n",
       "10    2.0     1.0 -10.733965\n",
       "11    8.0     1.0  -1.000000\n",
       "12    2.0     0.0  -8.880995\n",
       "13    4.0     1.0 -11.792185"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_policy_monte_carlo_prediction(policy, target_policy, phi, env, iterations, acting_episode_generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d8ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
